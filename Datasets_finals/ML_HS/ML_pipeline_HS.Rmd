---
title: "ML_Trial"
author: "Andrea,Gloria,Lorenzo,Thomas"
date: "`r Sys.Date()`"
output: html_document
---
# Preparatory steps
  Loading packages
  Store package names in a vectors for ease of access and to load them easily 
```{r Setup, message=FALSE, warning=FALSE}

# Define a list of packages to be loaded
PACKAGES <- c(
  "randomForest",    # For random forest modeling
  "caret",           # For machine learning model training and evaluation
  "gmodels",         # Provides tools for model diagnostics
  "yardstick",       # For model evaluation metrics
  "ggplot2",         # For data visualization
  "corrplot",        # For correlation plot visualization
  "tidyverse",       # Collection of packages for data manipulation and visualization
  "factoextra",      # Additional tools for clustering and factor analysis
  "FactoMineR",      # For exploratory data analysis and multivariate analysis
  "glue",            # For string interpolation
  "plotly",          # Interactive plots
  "dplyr",           # To manage data frames more efficiently
  "kernlab",         # Kernel-based machine learning methods
  "readr",           # For reading and parsing data
  "e1071",           # For SVM modeling and much MORE!
  "class",           # Various classification methods
  "MASS",            # Modern Applied Statistics with S
  "glmnet",          # For the LASSO regression 
  "class",           # For the KNN method
  "xgboost"          # For the XGBoost (Extreme Gradient Boosting) method
  
)

# Load the specified packages
invisible(lapply(PACKAGES, library, character.only = TRUE))


```

## Managing the data to use

```{r Data to use, warning=FALSE}
# Load the data
setwd("D:/VarieTHOM/University/QCB/3_SEMESTRE/Data Mining/Laboratory (Blanzieri)/0_PROJECT/Datasets_finals/ML_HS")
ML_data <- data.frame(read_csv("ML_HS.csv", col_names = TRUE))


# Set rownames
#ML_data <- subset(ML_data, select = -((ncol(ML_data) - 4):ncol(ML_data)))
row.names(ML_data) <- ML_data$...1

# Eliminate the columns that are not needed
my_data <- subset(ML_data, select = -c(...1,risk))
write.csv(my_data, "my_data.csv", row.names = TRUE)
#my_data$risk...127 <- NULL

# Subset data frame by condition
#df_Tumor <- subset(my_data, C_T == "Tumor")
```

## Number coating

```{r Current otimal way  }
# Getting ready to do predictions
# Number coating the values
## Specify the columns to be label encoded
columns_to_encode <- c("type", "C_T", "Cell_type")
columns_to_encode2 <- c("type", "C_T")

# Create a new data frame without rows where 'Cell_type' has value "Unknown"
my_data_train <- subset(my_data, Cell_type != "Unkown")

# Convert specified columns to factor type
my_data_train <- my_data_train %>% mutate_at(columns_to_encode, as.factor)

# Display levels of 'Cell_type'
levels(my_data_train$Cell_type)

# Convert specified columns to factor type for the entire dataset
my_data_encoded <- my_data %>% mutate_at(columns_to_encode, as.factor)


# Do the number coating as numbers
encoded <- my_data_encoded %>% mutate_at(columns_to_encode, as.numeric)
train_data <- my_data_train %>% mutate_at(columns_to_encode, as.numeric)

# Create a dictionary-like structure to store the labels
## The order corresponds to the number
my_levels <- list(
  type = levels(my_data_encoded$type),
  C_T = levels(my_data_encoded$C_T),
  Cell_type = levels(my_data_encoded$Cell_type)
)






# Split the data sets to start the following analysis
```


```{r Split the data}

# Set the seed for reproducibility
#set.seed(1236)

# Create an index for splitting the data
index <- createDataPartition(my_data_train$Cell_type, p = 0.7, list = FALSE)

# Create the training set
train <- my_data_train[index, ]

# Create the testing set
test <- my_data_train[-index, ]

# Create an index for splitting the data
index_numeric <- createDataPartition(train_data$Cell_type, p = 0.7, list = FALSE)

# Create the numeric training set
train_n <- train_data[index_numeric, ]

# Create the numeric testing set
test_n <- train_data[-index_numeric, ]
```

# Random Forest:

  -Type: Supervised Learning (Classification or Regression)
  
  -Description: Random Forest is an ensemble learning method that constructs a multitude of decision trees during training and outputs the mode of         the classes (classification) or the mean prediction (regression) of the individual trees. It is known for its robustness and ability to handle       complex data.
  -Working:
    Random Forest builds multiple decision trees during training, each based on a random subset of the features and a random subset of the training      data.
    Each tree "votes" on the class for a given input, and the mode (classification) or average (regression) of these votes becomes the final             prediction.
    This ensemble approach often leads to improved generalization and robustness compared to individual decision trees.

```{r Optimise the RF}
# Set seed for reproducibility
#set.seed(1236)

# Tune the Random Forest model
model_tuned <- tuneRF(
               x = my_data_train,                   # Define predictor variables
               y = my_data_train$Cell_type,  # Define response variable
               ntreeTry = 500,                      # Number of trees to try
               mtryStart = 4,                       # Starting value for mtry (number of variables randomly sampled as candidates at each split)
               stepFactor = 1.5,                    # Factor by which mtry is multiplied in each iteration
               improve = 0.001,                      # Minimum improvement in node impurity for a split to occur
               trace = FALSE,                       # Don't show real-time progress
               plot = TRUE,                         # Plot error rates during tuning
               importance = FALSE,                   # Calculate variable importance
               splitrule = "gini"
               )

```

# Fit the Random Forest model

```{r Fit the Random Forest model}
# Fit the Random Forest model
set.seed(1232)
model_RF <- randomForest(Cell_type ~ ., 
                      data = train,
                      ntree = 800,
                      splitrule = "gini",
                      importance = TRUE,
                      mtry = 13
)

# Create a copy of the data for predictions
predicted_RF <- my_data_encoded

# Make predictions on the test set
test_RF <- predict(model_RF, newdata = test)

# Fit another Random Forest model on the entire training dataset
model_RF2 <- randomForest(Cell_type ~ ., 
                      data = my_data_train,
                      ntree = 800,
                      splitrule = "gini",
                      importance = TRUE,
                      mtry = 13
)

# Make predictions on the new data
Prediction_RF <- predict(model_RF2, newdata = predicted_RF)

# View predictions
predict_RF <- data.frame(Prediction_RF)
predicted_RF$predicted_rf <- predict_RF$Prediction_RF
setwd("D:/VarieTHOM/University/QCB/3_SEMESTRE/Data Mining/Laboratory (Blanzieri)/0_PROJECT/Datasets_finals/ML_HS")
write.csv(predicted_RF, "Predict_rf.csv", row.names = TRUE)

# Display fitted models
print(model_RF)   # Using print for a more informative display
print(model_RF2)

```

# First basic plots for RF

```{r Plots and metrics for RF, warning=FALSE}

# Plot the test MSE by the number of trees
#plot(model_RF2, main = "Test MSE by Number of Trees", type = "l", pch = 16)

# Produce a variable importance plot
varImpPlot(model_RF2, main = "Variable Importance Plot", col = "darkgreen")

# Partial dependence plots illustrate the relationship between a specific feature and the predicted outcome while holding other features constant.
# They are useful for understanding the effect of individual features on the model predictions.
#partialPlot(model_RF2, train, PC7, col = "darkgreen", )

# Create a confusion matrix
conf_matrix <- table(test_RF, test$Cell_type)
#conf_matrix

cm <- confusionMatrix(test_RF, test$Cell_type)
cm

# Calculate metrics
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
precision <- diag(conf_matrix) / rowSums(conf_matrix)
recall <- diag(conf_matrix) / colSums(conf_matrix)
f1_score <- 2 * (precision * recall) / (precision + recall)

# Print the metrics
cat("\n","Accuracy:", accuracy, "\n")
cat("Precision:", mean(precision, na.rm = TRUE), "\n")
cat("Recall:", mean(recall, na.rm = TRUE), "\n")
cat("F1 Score:", mean(f1_score, na.rm = TRUE), "\n")

# Create a bar plot for precision, recall, and F1 score
metrics_df_RF <- data.frame(Metric = c("Accuracy","Precision", "Recall", "F1 Score"),
                         Value = c(mean(accuracy, na.rm = TRUE),
                                   mean(precision, na.rm = TRUE),
                                   mean(recall, na.rm = TRUE),
                                   mean(f1_score, na.rm = TRUE)))

metrics_df_RF

```

## K-Nearest Neighbors (KNN) Model:

  -Type: Supervised Learning (Classification or Regression)
  -Description: KNN is a simple algorithm that classifies a new data point based on the majority class of its k-nearest neighbors in the feature         space. It is a non-parametric, instance-based learning algorithm.
  
  -Working:
    KNN classifies a data point based on the class labels of its k-nearest neighbors in the feature space.
    The distance metric (usually Euclidean distance) is used to measure the proximity between data points.
    The algorithm then assigns the majority class among the k-nearest neighbors to the new data point.

```{r K-Nearest Neighbors with class library training}
set.seed(1234)
# Separate features and labels
train_features <- train_n[, -ncol(train)]  # All columns except the last one
train_labels <- train_n$Cell_type    # The last column

# Fit the KNN model
k_value <- 5  # You can choose the value of k
knn_model <- knn(train_features, test_n[, -which(names(test_n) == "Cell_type")], train_labels, k = k_value)
```


```{r Plots and metrix for K-Nearest Neighbors}
# Confusion Matrix
conf_matrix <- table(Actual = test_n$Cell_type, Predicted = knn_model)
conf_matrix

# Calculate metrics from the confusion matrix
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
precision <- diag(conf_matrix) / rowSums(conf_matrix)
recall <- diag(conf_matrix) / colSums(conf_matrix)
f1_score <- 2 * (precision * recall) / (precision + recall)

# Print the metrics
cat("Accuracy:", accuracy, "\n")
cat("Precision:", mean(precision, na.rm = TRUE), "\n")
cat("Recall:", mean(recall, na.rm = TRUE), "\n")
cat("F1 Score:", mean(f1_score, na.rm = TRUE), "\n")

# Create a data frame for precision, recall, and F1 score
metrics_df_KNN <- data.frame(Metric = c("Accuracy","Precision", "Recall", "F1 Score"),
                         Value = c(mean(accuracy, na.rm = TRUE),
                                   mean(precision, na.rm = TRUE),
                                   mean(recall, na.rm = TRUE),
                                   mean(f1_score, na.rm = TRUE)))
metrics_df_KNN


```

```{r K-Nearest Neighbors prediction, warning=FALSE}
set.seed(1234)
# Separate features and labels
train_features <- train_data[, -ncol(train_data)]  # All columns except the last one
train_labels <- train_data$Cell_type    # The last column

# Fit the KNN model
k_value <- 5  # You can choose the value of k
knn_model <- knn(train_features, encoded[, -which(names(encoded) == "Cell_type")], train_labels, k = k_value)

# Create a data frame with KNN predictions
predictions_knn <- data.frame(as.numeric(knn_model))

# Rename the columns for clarity
predictions_knn$knn_model <- predictions_knn$as.numeric.knn_model.
predictions_knn$as.numeric.knn_model. <- NULL

# Combine the KNN predictions with the original encoded data
predicted_KNN <- encoded
predicted_KNN$predicted_Knn <- predictions_knn$knn_model

#Write the results
setwd("D:/VarieTHOM/University/QCB/3_SEMESTRE/Data Mining/Laboratory (Blanzieri)/0_PROJECT/Datasets_finals/ML_HS")
write.csv(predicted_KNN, "Predict_Knn.csv", row.names = TRUE)
```


# XGBoost (Extreme Gradient Boosting):

  -Type: Ensemble Learning (boosting method)
  -Description: XGBoost is a powerful and efficient implementation of gradient boosting. It sequentially adds weak learners (usually decision trees)     to the model, each correcting errors of the previous one.
  -Working: It optimizes a loss function and includes regularization terms to avoid overfitting. The final prediction is a weighted sum of the           predictions from all the weak learners.

```{r XGBoost (Extreme Gradient Boosting), }
set.seed(1235)

# Step 1: Prepare the data
# Ensure that the last column in 'train_n' is the variable you want to predict
# For XGBoost, the response variable should be a factor for classification problems
# Extract predictor variables and response variable
X_train <- as.matrix(train_n[, -which(names(train_n) == "Cell_type")])  # Exclude the response variable
y_train <- as.matrix(train_n$Cell_type) -1   # Response variable

# Transform the matrix into an xgb.DMatrix
X_train <- dmatrix <- xgb.DMatrix(data = X_train, label = y_train)

# Step 2: Train the XGBoost model
xgb_model <- xgboost(data = X_train, 
                     objective = "multi:softmax", 
                     num_class = 4,
                     max.depth = 9,          # Maximum depth of each tree in the boosting process
                     nthread = 15,            # Number of threads to use during training
                     nrounds = 800,            # Number of boosting rounds (number of trees to build)
                     verbose = 0)



# Step 3: Make predictions on the new data
X_predict <- test_n[, -ncol(test_n)]  # Exclude the response variable

# Predict the class labels 
predictions_xgb <- data.frame(predict(xgb_model, as.matrix(X_predict)) +1)

predictions_xgb$predicted_xgb<- predictions_xgb$predict.xgb_model..as.matrix.X_predict.....1

predictions_xgb$predict.xgb_model..as.matrix.X_predict.....1<-NULL

#print(xgb_model)
summary(xgb_model)
```

```{r Plots and metrix for the XGBoost (Extreme Gradient Boosting)}
# Create a confusion matrix
conf_matrix <- table(predictions_xgb$predicted_xgb, test_n$Cell_type)
conf_matrix

# Calculate metrics
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
precision <- diag(conf_matrix) / rowSums(conf_matrix)
recall <- diag(conf_matrix) / colSums(conf_matrix)
f1_score <- 2 * (precision * recall) / (precision + recall)

# Print the metrics
cat("Accuracy:", accuracy, "\n")
cat("Precision:", mean(precision, na.rm = TRUE), "\n")
cat("Recall:", mean(recall, na.rm = TRUE), "\n")
cat("F1 Score:", mean(f1_score, na.rm = TRUE), "\n")

# Create a bar plot for precision, recall, and F1 score
metrics_df_xgb <- data.frame(Metric = c("Accuracy","Precision", "Recall", "F1 Score"),
                         Value = c(mean(accuracy, na.rm = TRUE),
                                   mean(precision, na.rm = TRUE),
                                   mean(recall, na.rm = TRUE),
                                   mean(f1_score, na.rm = TRUE)))

metrics_df_xgb
```

```{r XGB model on the full training set to predict}
set.seed(1235)
# Create a matrix of predictors
X <- as.matrix(train_data[, -which(names(train_data) == "Cell_type")])

# Convert the response variable to a numeric vector
y <- as.numeric(train_data$Cell_type)

Y_minusuno = as.matrix(train_data$Cell_type) -1

# Transform the matrix into an xgb.DMatrix
X <- dmatrix <- xgb.DMatrix(data = X, label = Y_minusuno)

xgb_model2 <- xgboost(data = X, 
                     
                     objective = "multi:softmax", 
                     num_class = 4,
                     max.depth = 13,          # Maximum depth of each tree in the boosting process
                     nthread = 15,            # Number of threads to use during training
                     nrounds = 1000,            # Number of boosting rounds (number of trees to build)
                     verbose = 0)
print(xgb_model2)
summary(xgb_model2)

#  Make predictions on the new data
xgb_predict <- encoded[, -ncol(encoded)]  # Exclude the response variable

# Predict the class labels 
xgb_predictions <- data.frame(predict(xgb_model2, as.matrix(xgb_predict)) +1)

xgb_predictions$predicted_xgb<- xgb_predictions$predict.xgb_model2..as.matrix.xgb_predict.....1

xgb_predictions$predict.xgb_model2..as.matrix.xgb_predict.....1<-NULL

xgb_predict$Cell_type <- encoded$Cell_type
xgb_predict$predicted_xgb<-xgb_predictions$predicted_xgb

setwd("D:/VarieTHOM/University/QCB/3_SEMESTRE/Data Mining/Laboratory (Blanzieri)/0_PROJECT/Datasets_finals/ML_HS")
write.csv(xgb_predict, "Predict_xgb.csv", row.names = TRUE)

```

# Naive Bayes:

  -Type: Supervised Learning (classification)
  -Description: Naive Bayes is a probabilistic classifier based on Bayes' theorem. It assumes that the features are conditionally independent given      the class label, which is a strong and often unrealistic assumption.
  -Working: It calculates the probability of each class given a set of features and selects the class with the highest probability as the predicted      class. Naive Bayes is computationally efficient and works well in practice, especially for text classification.

```{r Naive Bayes Model, warning=FALSE}
#set.seed(1234)
# Step 1: Prepare Data
features_train <- train[, -ncol(train)]
labels_train <- train[, ncol(train)]

features_predict <- test[, -ncol(test)]

Predict_nb <- my_data_encoded[, -ncol(my_data_encoded)]

# Step 2: Train Naive Bayes Model with Tuned Parameter
nb_model <- naiveBayes(features_train, labels_train, laplace = 0.001,usekernel = TRUE)

nb_model2 <- naiveBayes(train_data[, -ncol(train_data)], train_data[, ncol(train_data)], laplace = 0.001,usekernel = TRUE)

# Step 3: Make Predictions
predictions_nb <- predict(nb_model, features_predict)
predictions_nb2 <- data.frame(predict(nb_model, Predict_nb))

predictions_nb2$predicted_nb <- predictions_nb2$predict.nb_model..Predict_nb.
predictions_nb2$predict.nb_model..Predict_nb. <- NULL
Predict_nb$Cell_type <- my_data_encoded$Cell_type
Predict_nb$predicted_nb <- predictions_nb2$predicted_nb
setwd("D:/VarieTHOM/University/QCB/3_SEMESTRE/Data Mining/Laboratory (Blanzieri)/0_PROJECT/Datasets_finals/ML_HS")
write.csv(Predict_nb, "Predict_nb.csv", row.names = TRUE)
```

```{r Plots and metrics for Naive Bayes}
set.seed(1234)
# Create a confusion matrix
conf_matrix <- table(predictions_nb, test_n$Cell_type)
conf_matrix
# Calculate metrics
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
precision <- diag(conf_matrix) / rowSums(conf_matrix)
recall <- diag(conf_matrix) / colSums(conf_matrix)
f1_score <- 2 * (precision * recall) / (precision + recall)

# Print the metrics
cat("Accuracy:", accuracy, "\n")
cat("Precision:", mean(precision, na.rm = TRUE), "\n")
cat("Recall:", mean(recall, na.rm = TRUE), "\n")
cat("F1 Score:", mean(f1_score, na.rm = TRUE), "\n")

# Create a DF for precision, recall, and F1 score
metrics_df_nb <- data.frame(Metric = c("Accuracy","Precision", "Recall", "F1 Score"),
                         Value = c(mean(accuracy, na.rm = TRUE),
                                   mean(precision, na.rm = TRUE),
                                   mean(recall, na.rm = TRUE),
                                   mean(f1_score, na.rm = TRUE)))
metrics_df_nb

```